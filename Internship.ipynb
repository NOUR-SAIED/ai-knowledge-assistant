{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQZU7rlnRIJF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEJIMqipzR7-"
      },
      "source": [
        "#DATA PREPERATION PIPELINE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ji3z_iskRYgo",
        "outputId": "fbae4457-d438-4c82-b0c0-9028d70a2942"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vv0YDE6LRZVe"
      },
      "outputs": [],
      "source": [
        "!pip install beautifulsoup4 langchain sentence-transformers chromadb -q\n",
        "import os\n",
        "import glob\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IV9YX6tZ9gES"
      },
      "outputs": [],
      "source": [
        "# The DEFINITIVE Document Loading Logic (Using Your Findings)\n",
        "\n",
        "def load_and_clean_document(file_path):\n",
        "    \"\"\"\n",
        "    Loads an HTML file, finds the title from anywhere on the page,\n",
        "    and extracts text ONLY from the specific content 'treasure chest'.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            html_content = f.read()\n",
        "\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "        # --- Robot #1: The Title Finder (Searches the whole document) ---\n",
        "        # This logic is perfect and doesn't need to change.\n",
        "        title = ''\n",
        "        if soup.title and soup.title.string:\n",
        "            title = soup.title.string.strip()\n",
        "        elif soup.find('h1'):\n",
        "            title = soup.find('h1').get_text(strip=True)\n",
        "        else:\n",
        "            filename = os.path.basename(file_path)\n",
        "            title = filename.replace('_', ' ').replace('-', ' ').replace('.html', '').capitalize()\n",
        "\n",
        "        # --- Robot #2: The Content Finder (Goes to the exact treasure chest) ---\n",
        "        # We target the specific div you found. We can use both id and class\n",
        "        # for an extremely precise search.\n",
        "        content_chest = soup.find(id='main-content', class_='wiki-content group')\n",
        "\n",
        "        clean_text = ''\n",
        "        if content_chest:\n",
        "            # If we found our specific treasure chest, get text ONLY from it.\n",
        "            clean_text = content_chest.get_text(separator=' ', strip=True)\n",
        "        else:\n",
        "            # Fallback in case a rare page has a different structure.\n",
        "            # We'll just grab everything we can.\n",
        "            clean_text = soup.get_text(separator=' ', strip=True)\n",
        "\n",
        "        # --- Assembling our clean \"Recipe Card\" ---\n",
        "        return {\n",
        "            'title': title,\n",
        "            'text': clean_text,\n",
        "            'source_file': os.path.basename(file_path)\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file {file_path}: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRwRnrIJTWJ4",
        "outputId": "50729b77-156d-4ffe-9732-46bbeee9d682"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 479 HTML files to process.\n",
            "\n",
            "--- Successfully Processed First File ---\n",
            "Source File: Flow-Explorer-application_324469701.html\n",
            "Found Title: PS Team Master Authorizers : Flow Explorer application\n",
            "\n",
            "--- First 500 Characters of Cleaned Text ---\n",
            "Functional Requirements Requirement Description Show granted Authz List the Entry authz with missing Exit List the Exit authz with missing Entry Show declined Authz List the not granted Entry/Exit authz with response message, this includes both use cases Declined Authz NOTFOUND customer Filtering Filter includes simple and combined filter conditions Manual Booking Operator should be able to close Opened Entry / Exit Authz through manual booking I18N Localization of the table header & table conte\n"
          ]
        }
      ],
      "source": [
        "base_folder_path = '/content/drive/My Drive/AI_Internship_Project/data/'\n",
        "\n",
        "\n",
        "html_files = glob.glob(os.path.join(base_folder_path, '**', '*.html'), recursive=True)\n",
        "\n",
        "print(f\"Found {len(html_files)} HTML files to process.\")\n",
        "\n",
        "# Process just ONE file to test our new function\n",
        "if html_files:\n",
        "    first_file_path = html_files[0]\n",
        "    document_data = load_and_clean_document(first_file_path)\n",
        "\n",
        "    if document_data:\n",
        "        print(\"\\n--- Successfully Processed First File ---\")\n",
        "        print(f\"Source File: {document_data['source_file']}\")\n",
        "        print(f\"Found Title: {document_data['title']}\")\n",
        "        print(\"\\n--- First 500 Characters of Cleaned Text ---\")\n",
        "        print(document_data['text'][:500])\n",
        "else:\n",
        "    print(\"No HTML files were found. Please check your `base_folder_path`.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTTRrr1xUGx9",
        "outputId": "2041202a-99b3-4cd5-fc47-c75ca3dd90b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Starting Part 1: Loading and Cleaning All Documents ---\n",
            "Found 479 HTML files.\n",
            "Successfully loaded and cleaned 479 documents in 24.55 seconds.\n",
            "\n",
            "--- Starting Part 2: Chunking All Documents ---\n",
            "Created a total of 1400 chunks from 479 documents in 0.37 seconds.\n",
            "\n",
            "--- Sample of the first 3 chunks ---\n",
            "\n",
            "--- Chunk 1 ---\n",
            "Content: Functional Requirements Requirement Description Show granted Authz List the Entry authz with missing Exit List the Exit authz with missing Entry Show declined Authz List the not granted Entry/Exit aut...\n",
            "Metadata: {'title': 'PS Team Master Authorizers : Flow Explorer application', 'source': 'Flow-Explorer-application_324469701.html'}\n",
            "\n",
            "--- Chunk 2 ---\n",
            "Content: Consumers details are displayed in topic transactionmanager.event.1 with Entry&Exit events as shown: Entry Event { \"source\": \"com.scheidtbachmann.phfa.message.TransactionMessage\", \"time\": \"2024-03-26T...\n",
            "Metadata: {'title': 'PS Team Master Authorizers : Kafka event consumer details', 'source': 'Kafka-event-consumer-details_339791636.html'}\n",
            "\n",
            "--- Chunk 3 ---\n",
            "Content: null, \"country\": \"DEU\", \"region\": \"MG\" } }, { \"id\": \"transaction_v2 - 262ce362-91f3-3aad-b623-f450ec9f04d7\", \"mediaType\": \"QRCODE\", \"encoding\": \"NATIVE\", \"mediaLpn\": { \"qualityFactor\": null, \"country\"...\n",
            "Metadata: {'title': 'PS Team Master Authorizers : Kafka event consumer details', 'source': 'Kafka-event-consumer-details_339791636.html'}\n"
          ]
        }
      ],
      "source": [
        "# THE FULL PIPELINE\n",
        "\n",
        "import time\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        " #Part 1: Load all 479 documents\n",
        "print(\"--- Starting Part 1: Loading and Cleaning All Documents ---\")\n",
        "start_time = time.time()\n",
        "\n",
        "base_folder_path = '/content/drive/My Drive/AI_Internship_Project/data/'\n",
        "html_files = glob.glob(os.path.join(base_folder_path, '**', '*.html'), recursive=True)\n",
        "print(f\"Found {len(html_files)} HTML files.\")\n",
        "\n",
        "documents = []\n",
        "for file_path in html_files:\n",
        "    doc_data = load_and_clean_document(file_path)\n",
        "    if doc_data:\n",
        "        documents.append(doc_data)\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Successfully loaded and cleaned {len(documents)} documents in {end_time - start_time:.2f} seconds.\")\n",
        "\n",
        "\n",
        "# Part 2: Chunk the clean documents for the AI\n",
        "print(\"\\n--- Starting Part 2: Chunking All Documents ---\")\n",
        "start_time = time.time()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200\n",
        ")\n",
        "\n",
        "all_chunks = []\n",
        "for doc in documents:\n",
        "    # We only chunk documents that have a meaningful amount of text\n",
        "    if len(doc['text']) > 100:\n",
        "        chunks = text_splitter.split_text(doc['text'])\n",
        "        for chunk_text in chunks:\n",
        "            all_chunks.append({\n",
        "                'page_content': chunk_text,\n",
        "                'metadata': {\n",
        "                    'title': doc['title'],\n",
        "                    'source': doc['source_file']\n",
        "                }\n",
        "            })\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Created a total of {len(all_chunks)} chunks from {len(documents)} documents in {end_time - start_time:.2f} seconds.\")\n",
        "\n",
        "# Verification Step\n",
        "print(\"\\n--- Sample of the first 3 chunks ---\")\n",
        "for i, chunk in enumerate(all_chunks[:3]):\n",
        "    print(f\"\\n--- Chunk {i+1} ---\")\n",
        "    print(f\"Content: {chunk['page_content'][:200]}...\")\n",
        "    print(f\"Metadata: {chunk['metadata']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFKi7P_tzymN",
        "outputId": "08f8cc20-0ccf-4473-e0a8-6ba048b3ad7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Starting Part 3: Embedding and Indexing ---\n",
            "  Processed batch 1/44\n",
            "  Processed batch 2/44\n",
            "  Processed batch 3/44\n",
            "  Processed batch 4/44\n",
            "  Processed batch 5/44\n",
            "  Processed batch 6/44\n",
            "  Processed batch 7/44\n",
            "  Processed batch 8/44\n",
            "  Processed batch 9/44\n",
            "  Processed batch 10/44\n",
            "  Processed batch 11/44\n",
            "  Processed batch 12/44\n",
            "  Processed batch 13/44\n",
            "  Processed batch 14/44\n",
            "  Processed batch 15/44\n",
            "  Processed batch 16/44\n",
            "  Processed batch 17/44\n",
            "  Processed batch 18/44\n",
            "  Processed batch 19/44\n",
            "  Processed batch 20/44\n",
            "  Processed batch 21/44\n",
            "  Processed batch 22/44\n",
            "  Processed batch 23/44\n",
            "  Processed batch 24/44\n",
            "  Processed batch 25/44\n",
            "  Processed batch 26/44\n",
            "  Processed batch 27/44\n",
            "  Processed batch 28/44\n",
            "  Processed batch 29/44\n",
            "  Processed batch 30/44\n",
            "  Processed batch 31/44\n",
            "  Processed batch 32/44\n",
            "  Processed batch 33/44\n",
            "  Processed batch 34/44\n",
            "  Processed batch 35/44\n",
            "  Processed batch 36/44\n",
            "  Processed batch 37/44\n",
            "  Processed batch 38/44\n",
            "  Processed batch 39/44\n",
            "  Processed batch 40/44\n",
            "  Processed batch 41/44\n",
            "  Processed batch 42/44\n",
            "  Processed batch 43/44\n",
            "  Processed batch 44/44\n",
            "\n",
            "Successfully created and indexed the vector database in 229.80 seconds.\n",
            "The database contains 1400 chunks.\n",
            "\n",
            "--- Testing the Search Functionality ---\n",
            "Query: 'What are the functional requirements for authorization?'\n",
            "\n",
            "--- Top 3 Results ---\n",
            "\n",
            "--- Result 1 ---\n",
            "Text: be secured by authentication, in fact, we can think about 2 access types : Public items : doc pages, API pages, TRY IT button (mock mode) to test the api will return a mock and only a mock Private items: non public doc pages, sensitive API pages, TRY IT button (test mode) to test the api will run a real call to some TEST tenant How to secure the portal? V1 : basic path or page access : the idea is to set some pages as public and others as private via path or route protection. User must be authenticated to access certain page routes. Authentication can be handled by Keycloak Protect API access using KONG routes Basic pages access : can see full page/route or no access at all V2: advanced permission system similar to what we have in program manager Rôles definition: who can see what? Implement permissions at the application level: protect pages by role/permission requirement How to secure the API? V1: try it button will work as a mock, defined by the API spec, an example can be seen in\n",
            "Metadata: {'title': 'PS Team Master Authorizers : Online API portal', 'source': 'Online-API-portal_393487163.html'}\n",
            "\n",
            "--- Result 2 ---\n",
            "Text: permissions at the application level: protect pages by role/permission requirement How to secure the API? V1: try it button will work as a mock, defined by the API spec, an example can be seen in the proof of concept above. Mock prepared while designing the API, it should contain an example of response Anyone seeing the page can use the try it button V2: extend the authentication to support the try it button so that the user have the ability to run real calls to the APIs Try it can only be performed in a TEST tenant, that should be prepared and deployed Try it button is visible to allowed users only (refer to the permission system described above) Web portal should retrieved the authenticated user's token and insert that in the try it call as header to run a real API call ( can be performed using Open API Specs or a script) TEST tenant must be configured with users and rôles allowed to use the API's Other solutions Simpler/less effort, but offers less feature, the below solutions are\n",
            "Metadata: {'title': 'PS Team Master Authorizers : Online API portal', 'source': 'Online-API-portal_393487163.html'}\n",
            "\n",
            "--- Result 3 ---\n",
            "Text: Client ID : admin-realm-management\n",
            "                         Client Protocol : openId-connect \n",
            "                         Standard Flow Enabled : enabled   \n",
            "                         Implicit Flow Enabled : enabled\n",
            "                         Direct Access Grants Enabled: enabled\n",
            "Metadata: {'source': '64847067.html', 'title': 'PS Team Master Authorizers : Multi-tenancy : Migration Scenario'}\n"
          ]
        }
      ],
      "source": [
        "# THE AI CORE\n",
        "\n",
        "import chromadb\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import time\n",
        "\n",
        "print(\"--- Starting Part 3: Embedding and Indexing ---\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Step 1: Initialize the Embedding Model\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')\n",
        "\n",
        "# Step 2: Initialize the Vector Database\n",
        "db_path = '/content/drive/My Drive/AI_Internship_Project/chroma_db'\n",
        "client = chromadb.PersistentClient(path=db_path)\n",
        "\n",
        "# Step 3: Create a collection\n",
        "collection_name = \"confluence_docs\"\n",
        "if collection_name in [c.name for c in client.list_collections()]:\n",
        "    client.delete_collection(name=collection_name)\n",
        "\n",
        "collection = client.create_collection(name=collection_name)\n",
        "\n",
        "# Step 4: Add all the chunks to the database\n",
        "\n",
        "ids = [str(i) for i in range(len(all_chunks))]\n",
        "documents_to_embed = [chunk['page_content'] for chunk in all_chunks]\n",
        "metadatas_to_store = [chunk['metadata'] for chunk in all_chunks]\n",
        "\n",
        "# Embed the documents in batches for efficiency\n",
        "batch_size = 32\n",
        "for i in range(0, len(documents_to_embed), batch_size):\n",
        "    batch_docs = documents_to_embed[i:i+batch_size]\n",
        "    batch_ids = ids[i:i+batch_size]\n",
        "    batch_metadatas = metadatas_to_store[i:i+batch_size]\n",
        "\n",
        "    # The embedding model converts our text to vectors\n",
        "    embeddings = embedding_model.encode(batch_docs).tolist()\n",
        "\n",
        "    # Add the batch to the collection\n",
        "    collection.add(\n",
        "        embeddings=embeddings,\n",
        "        documents=batch_docs,\n",
        "        metadatas=batch_metadatas,\n",
        "        ids=batch_ids\n",
        "    )\n",
        "    print(f\"  Processed batch {i//batch_size + 1}/{(len(documents_to_embed)//batch_size) + 1}\")\n",
        "\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"\\nSuccessfully created and indexed the vector database in {end_time - start_time:.2f} seconds.\")\n",
        "print(f\"The database contains {collection.count()} chunks.\")\n",
        "\n",
        "# Verification Step\n",
        "print(\"\\n--- Testing the Search Functionality ---\")\n",
        "\n",
        "query_text = \"What are the functional requirements for authorization?\"\n",
        "results = collection.query(\n",
        "    query_texts=[query_text],\n",
        "    n_results=3 # Ask for the top 3 most relevant results\n",
        ")\n",
        "\n",
        "print(f\"Query: '{query_text}'\")\n",
        "print(\"\\n--- Top 3 Results ---\")\n",
        "for i, doc in enumerate(results['documents'][0]):\n",
        "    print(f\"\\n--- Result {i+1} ---\")\n",
        "    print(f\"Text: {doc}\")\n",
        "    print(f\"Metadata: {results['metadatas'][0][i]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 480,
          "referenced_widgets": [
            "de70d8d4a43c4efe866e1c6533137669",
            "66a6fb2b80ab4b399ff0ac47ba5590ae",
            "e735c59478464ae8b8df3a7b9c06a132",
            "2361f53e62134c438189bded8da56692",
            "610c2ff33aa743b69d78282848d381e2",
            "57f3eeea15c74659ae5d91d351265b75",
            "8c76b9d2e3d0448e86e1d23ca20a1a83",
            "807a54be2a5746ec881164a16fb40295",
            "4c5907ea1a5842d780f0789b5988b6de",
            "395187b4296547588cb015b928163a1d",
            "f63e0de1f80c4aaea1c7268f885081b3",
            "49d9cf63d6104db1820dafe73bded7cc",
            "4b37e657d72a457caeb5108e85d76c34",
            "91942e5269b7461897f0190b33353cf2",
            "d3062c91a4334deeb3d1165b71002809",
            "085c5cd59e3b4ec68aaa64373b6abf1a",
            "60e91a7e268440998993dc1e89b683fc",
            "320647261c6241418411ab15629c9c84",
            "43b89ad0f0934d308300d0151a2ce271",
            "dc1fd9ce1dc0460e8d31eaa4d66d25c0",
            "e207dec9231f4343b089cd713b6521f5",
            "086a19da7fff4e97ae003a217a04c350"
          ]
        },
        "id": "irUOnGIS8FoU",
        "outputId": "769bfd8b-bd6d-4dab-e673-b54fabae457a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ctransformers in /usr/local/lib/python3.11/dist-packages (0.2.27)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from ctransformers) (0.34.4)\n",
            "Requirement already satisfied: py-cpuinfo<10.0.0,>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from ctransformers) (9.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->ctransformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->ctransformers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->ctransformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->ctransformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->ctransformers) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->ctransformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->ctransformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->ctransformers) (1.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->ctransformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->ctransformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->ctransformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->ctransformers) (2025.8.3)\n",
            "--- Loading the LLM ---\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "de70d8d4a43c4efe866e1c6533137669",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "49d9cf63d6104db1820dafe73bded7cc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- LLM Loaded Successfully ---\n",
            "\n",
            "--- Testing the Full RAG Pipeline ---\n",
            "1. Starting retrieval for query: 'What are the functional requirements for authorization?'\n",
            "2. Retrieved context successfully.\n",
            "3. Generating response...\n",
            "4. Response generated.\n",
            "\n",
            "--- Final Generated Answer ---\n",
            "Based on the provided context, the functional requirements for authorization include:  1. Securing\n",
            "the portal by setting some pages as public and others as private via path or route protection. Users\n",
            "must be authenticated to access certain page routes. (V1) 2. Protecting pages by role/permission\n",
            "requirement at the application level. (V2) 3. Extending authentication to support the try it button\n",
            "so that users have the ability to run real calls to APIs in a TEST tenant. (V2) 4. Configuring the\n",
            "TEST tenant with users and roles allowed to use the API's. (V2) 5. Retrieving the authenticated\n",
            "user's token and inserting it in the try it call as a header to run a real API call. (V2) 6.\n",
            "Ignoring presence check parameter config to avoid Online Auth processing for Entry or Exit. (Access\n",
            "Governance) 7. Granting/Declining garage access for a parker based on Transaction Manager level,\n",
            "Sensor data consistency, Ignore presence check parameter, Car park full/free switch off, and Tariff\n",
            "at Exit not null. (Access Governance)  Sources: - \"How  Sources: - Online-API-portal_393487163.html\n",
            "- 64847067.html - Parking-Access-Governance_214604468.html - On-boarding-CS3_346318098.html\n"
          ]
        }
      ],
      "source": [
        "# Part 4: Building the Generation Engine\n",
        "\n",
        "# Step 1: Install the necessary library for the LLM\n",
        "!pip install ctransformers\n",
        "\n",
        "from ctransformers import AutoModelForCausalLM\n",
        "import textwrap\n",
        "\n",
        "# --- Part 4: Building the Generation Engine (Corrected) ---\n",
        "\n",
        "\n",
        "from ctransformers import AutoModelForCausalLM\n",
        "import textwrap\n",
        "\n",
        "# Step 2: Load the Open-Source LLM with Increased Context\n",
        "print(\"--- Loading the LLM ---\")\n",
        "llm = AutoModelForCausalLM.from_pretrained(\n",
        "    \"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\",\n",
        "    model_file=\"mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n",
        "    model_type=\"mistral\",\n",
        "    gpu_layers=0,\n",
        "    context_length=4096\n",
        ")\n",
        "print(\"--- LLM Loaded Successfully ---\")\n",
        "\n",
        "# Step 3: The RAG Function\n",
        "def get_rag_response(query, n_results=5):\n",
        "    \"\"\"\n",
        "    Takes a user query, retrieves relevant documents, and generates a response.\n",
        "    \"\"\"\n",
        "    print(f\"1. Starting retrieval for query: '{query}'\")\n",
        "    results = collection.query(\n",
        "        query_texts=[query],\n",
        "        n_results=n_results\n",
        "    )\n",
        "    retrieved_docs = results['documents'][0]\n",
        "    retrieved_metadata = results['metadatas'][0]\n",
        "\n",
        "    context = \"\\n\\n---\\n\\n\".join(retrieved_docs)\n",
        "    print(\"2. Retrieved context successfully.\")\n",
        "\n",
        "    prompt_template = f\"\"\"\n",
        "    [INST]\n",
        "    You are an expert technical assistant. Your task is to answer the user's question based ONLY on the following context from Confluence documentation.\n",
        "    If the context does not contain the answer, state that you cannot find the information in the provided documents.\n",
        "    Do not use any prior knowledge. Be concise and quote the source document when possible.\n",
        "\n",
        "    CONTEXT:\n",
        "    {context}\n",
        "    ---\n",
        "    QUESTION:\n",
        "    {query}\n",
        "    [/INST]\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"3. Generating response...\")\n",
        "    response = llm(prompt_template, max_new_tokens=256, temperature=0.1)\n",
        "\n",
        "    print(\"4. Response generated.\")\n",
        "\n",
        "    sources = list(set([meta['source'] for meta in retrieved_metadata]))\n",
        "    source_str = \"\\n\\nSources:\\n\" + \"\\n\".join(f\"- {source}\" for source in sources)\n",
        "\n",
        "    return response + source_str\n",
        "\n",
        "\n",
        "# Step 3: Create the Full RAG Function\n",
        "def get_rag_response(query, n_results=5):\n",
        "    \"\"\"\n",
        "    Takes a user query, retrieves relevant documents, and generates a response.\n",
        "    \"\"\"\n",
        "    print(f\"1. Starting retrieval for query: '{query}'\")\n",
        "    #  RETRIEVAL\n",
        "    # Get the top N most relevant chunks from our ChromaDB vector store\n",
        "    results = collection.query(\n",
        "        query_texts=[query],\n",
        "        n_results=n_results\n",
        "    )\n",
        "    retrieved_docs = results['documents'][0]\n",
        "    retrieved_metadata = results['metadatas'][0]\n",
        "\n",
        "    #  AUGMENTATION\n",
        "    # Combine the retrieved documents into a single context string\n",
        "    context = \"\\n\\n---\\n\\n\".join(retrieved_docs)\n",
        "    print(\"2. Retrieved context successfully.\")\n",
        "\n",
        "    #  GENERATION\n",
        "    # Create the prompt using our template\n",
        "    # The [INST] and [/INST] tags are specific to the Mistral Instruct model.\n",
        "    prompt_template = f\"\"\"\n",
        "    [INST]\n",
        "    You are an expert technical assistant. Your task is to answer the user's question based ONLY on the following context from Confluence documentation.\n",
        "    If the context does not contain the answer, state that you cannot find the information in the provided documents.\n",
        "    Do not use any prior knowledge. Be concise and quote the source document when possible.\n",
        "\n",
        "    CONTEXT:\n",
        "    {context}\n",
        "    ---\n",
        "    QUESTION:\n",
        "    {query}\n",
        "    [/INST]\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"3. Generating response...\")\n",
        "    # Pass the prompt to the LLM\n",
        "    response = llm(prompt_template, max_new_tokens=256, temperature=0.1)\n",
        "\n",
        "    print(\"4. Response generated.\")\n",
        "\n",
        "    # Add source citations\n",
        "    sources = list(set([meta['source'] for meta in retrieved_metadata]))\n",
        "    source_str = \"\\n\\nSources:\\n\" + \"\\n\".join(f\"- {source}\" for source in sources)\n",
        "\n",
        "    return response + source_str\n",
        "\n",
        "\n",
        "#  Verification Step\n",
        "print(\"\\n--- Testing the Full RAG Pipeline ---\")\n",
        "\n",
        "query_text = \"What are the functional requirements for authorization?\"\n",
        "final_answer = get_rag_response(query_text)\n",
        "\n",
        "print(\"\\n--- Final Generated Answer ---\")\n",
        "\n",
        "wrapped_answer = textwrap.fill(final_answer, width=100)\n",
        "print(wrapped_answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "cLE_iCVXygg9",
        "outputId": "4b8e63dd-383f-4d46-e5b9-c3c5db21c13f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Testing a 'How To' Question ---\n",
            "1. Starting retrieval for query: 'What do I do In case of virtual machine crashing repeatedly'\n",
            "2. Retrieved context successfully.\n",
            "3. Generating response...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3734727496.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- Testing a 'How To' Question ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mquery_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"What do I do In case of virtual machine crashing repeatedly\"\u001b[0m \u001b[0;31m# A completely different type of query\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfinal_answer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_rag_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- Final Generated Answer ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-179975543.py\u001b[0m in \u001b[0;36mget_rag_response\u001b[0;34m(query, n_results)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"3. Generating response...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;31m# Pass the prompt to the LLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_template\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"4. Response generated.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ctransformers/llm.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, prompt, max_new_tokens, top_k, top_p, temperature, repetition_penalty, last_n_tokens, seed, batch_size, threads, stop, stream, reset)\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ctransformers/llm.py\u001b[0m in \u001b[0;36m_stream\u001b[0;34m(self, prompt, max_new_tokens, top_k, top_p, temperature, repetition_penalty, last_n_tokens, seed, batch_size, threads, stop, reset)\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mincomplete\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         for token in self.generate(\n\u001b[0m\u001b[1;32m    571\u001b[0m             \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ctransformers/llm.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, tokens, top_k, top_p, temperature, repetition_penalty, last_n_tokens, seed, batch_size, threads, reset)\u001b[0m\n\u001b[1;32m    525\u001b[0m         \"\"\"\n\u001b[1;32m    526\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_inputs_for_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthreads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m             token = self.sample(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ctransformers/llm.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, tokens, batch_size, threads)\u001b[0m\n\u001b[1;32m    401\u001b[0m             )\n\u001b[1;32m    402\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mc_int\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         status = self.ctransformers_llm_batch_eval(\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mn_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#  test a new, process-oriented question\n",
        "print(\"\\n--- Testing a 'How To' Question ---\")\n",
        "query_text = \"What do I do In case of virtual machine crashing repeatedly\" # A completely different type of query\n",
        "final_answer = get_rag_response(query_text)\n",
        "\n",
        "print(\"\\n--- Final Generated Answer ---\")\n",
        "wrapped_answer = textwrap.fill(final_answer, width=100)\n",
        "print(wrapped_answer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gPlW6ggenQm-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "085c5cd59e3b4ec68aaa64373b6abf1a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "086a19da7fff4e97ae003a217a04c350": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2361f53e62134c438189bded8da56692": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_395187b4296547588cb015b928163a1d",
            "placeholder": "​",
            "style": "IPY_MODEL_f63e0de1f80c4aaea1c7268f885081b3",
            "value": " 1/1 [00:00&lt;00:00, 21.56it/s]"
          }
        },
        "320647261c6241418411ab15629c9c84": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "395187b4296547588cb015b928163a1d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43b89ad0f0934d308300d0151a2ce271": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49d9cf63d6104db1820dafe73bded7cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4b37e657d72a457caeb5108e85d76c34",
              "IPY_MODEL_91942e5269b7461897f0190b33353cf2",
              "IPY_MODEL_d3062c91a4334deeb3d1165b71002809"
            ],
            "layout": "IPY_MODEL_085c5cd59e3b4ec68aaa64373b6abf1a"
          }
        },
        "4b37e657d72a457caeb5108e85d76c34": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60e91a7e268440998993dc1e89b683fc",
            "placeholder": "​",
            "style": "IPY_MODEL_320647261c6241418411ab15629c9c84",
            "value": "Fetching 1 files: 100%"
          }
        },
        "4c5907ea1a5842d780f0789b5988b6de": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "57f3eeea15c74659ae5d91d351265b75": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60e91a7e268440998993dc1e89b683fc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "610c2ff33aa743b69d78282848d381e2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66a6fb2b80ab4b399ff0ac47ba5590ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57f3eeea15c74659ae5d91d351265b75",
            "placeholder": "​",
            "style": "IPY_MODEL_8c76b9d2e3d0448e86e1d23ca20a1a83",
            "value": "Fetching 1 files: 100%"
          }
        },
        "807a54be2a5746ec881164a16fb40295": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c76b9d2e3d0448e86e1d23ca20a1a83": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91942e5269b7461897f0190b33353cf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43b89ad0f0934d308300d0151a2ce271",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dc1fd9ce1dc0460e8d31eaa4d66d25c0",
            "value": 1
          }
        },
        "d3062c91a4334deeb3d1165b71002809": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e207dec9231f4343b089cd713b6521f5",
            "placeholder": "​",
            "style": "IPY_MODEL_086a19da7fff4e97ae003a217a04c350",
            "value": " 1/1 [00:00&lt;00:00, 11.25it/s]"
          }
        },
        "dc1fd9ce1dc0460e8d31eaa4d66d25c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "de70d8d4a43c4efe866e1c6533137669": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_66a6fb2b80ab4b399ff0ac47ba5590ae",
              "IPY_MODEL_e735c59478464ae8b8df3a7b9c06a132",
              "IPY_MODEL_2361f53e62134c438189bded8da56692"
            ],
            "layout": "IPY_MODEL_610c2ff33aa743b69d78282848d381e2"
          }
        },
        "e207dec9231f4343b089cd713b6521f5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e735c59478464ae8b8df3a7b9c06a132": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_807a54be2a5746ec881164a16fb40295",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4c5907ea1a5842d780f0789b5988b6de",
            "value": 1
          }
        },
        "f63e0de1f80c4aaea1c7268f885081b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}